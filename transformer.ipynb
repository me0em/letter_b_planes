{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d5df28-6f05-4aba-a6f6-8cf703cf9a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2673156bd50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import einops\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "\n",
    "from utils import PlaneSet2Neurons, configurate_xy_tensors\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from IPython.display import display\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14178094-e7e3-4058-8a5c-b243505bd691",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dcf0d93-18b0-4cfb-8cae-ae6186c300ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03068689-76ce-48b8-9ad9-8457f68478a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters amount: 86 567 656\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "print(\"Trainable parameters amount: {:n}\".format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff09e07-d522-48e0-bb0a-f543035072e5",
   "metadata": {},
   "source": [
    "##### Replace the latest feed-forward layer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099655b1-12e1-4f30-98b6-a08f4cace186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=1000, bias=True)\n",
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier)\n",
    "model.classifier = nn.Linear(768, 2)\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19523d35-e07e-4f03-8cc5-b1ca5c7269fa",
   "metadata": {},
   "source": [
    "##### Fine tune the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b84cf4-95cc-47dd-b911-fb461b696ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 2\n",
    "batch_size = 5\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device=device)\n",
    "\n",
    "# data\n",
    "# --------------------------\n",
    "csv_path = r\"../train\"\n",
    "images_path = r\"../avia-train/\"\n",
    "with open(csv_path, \"r\") as file:\n",
    "    data = pd.read_csv(file)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train_df = data[msk]\n",
    "test_df = data[~msk]\n",
    "train_dataset = PlaneSet2Neurons(images_path, train_df)\n",
    "test_dataset = PlaneSet2Neurons(images_path, test_df)\n",
    "train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0fa9d4-1c15-4c07-b6b4-142641f1da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch_to_pil(batch):\n",
    "    return [\n",
    "        torchvision.transforms.ToPILImage()(i)\n",
    "        for i in batch\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e43aa-de1d-4649-b647-87c5d5f2dfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_dict = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train_accumulator = []\n",
    "\n",
    "    for ind, (x, y) in enumerate(train):\n",
    "        x, y = configurate_xy_tensors(x, y)\n",
    "        x = convert_batch_to_pil(x)\n",
    "        x = feature_extractor(images=x, return_tensors=\"pt\")\n",
    "        y_hat = model(x.pixel_values.cuda())\n",
    "        logits = y_hat.logits\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y = einops.rearrange(y, \"b h w -> b (h w)\")\n",
    "        loss = distance(logits, y)\n",
    "        diff = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train_accumulator.append(diff)\n",
    "        \n",
    "        del x, y, y_hat, logits, loss\n",
    "        \n",
    "    loss_dict[epoch+1] = np.mean(loss_train_accumulator)\n",
    "\n",
    "    print('epoch [{}/{}], loss: {:.5f}'.format(epoch+1, num_epochs, np.mean(loss_train_accumulator)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681eecbb-22a5-4057-968f-7613f4798a7f",
   "metadata": {},
   "source": [
    "# прогонка одной пикчи через модель (отсюда перекинуть нормально в цикл обучения логику)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff79b59-1b35-4f80-b1fd-ee09e9862398",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../avia-train/0a3e36d7-877b-49a1-85dd-ce1e2d018460.png\"\n",
    "img.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0161d6-8660-41b6-808f-3e9b9c07d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(images = np.array(img.open(path)), return_tensors=\"pt\")\n",
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504c3b8-00c8-4590-b251-8463126c352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(img.open(path)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc14eff-58ab-45fc-9f8c-2b58b7779873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = einops.rearrange(\n",
    "    (inputs[\"pixel_values\"].squeeze(0).numpy() * 255).astype(np.uint8),\n",
    "    \"c h w -> h w c\"\n",
    ")\n",
    "\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21f646-9b50-41d1-ae1e-cfe9ef0d748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.fromarray(\n",
    "    tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ac7bc-224f-48ec-abb2-57c292e523ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80512bf-0d12-4813-aebf-72cf68939310",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bc5fe-a206-477b-a03c-6d7b70910bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45eb86d-dc54-4bb1-9d2c-535eebaf54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257c3ab-79c8-46ca-aaac-31d5bbf2861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5fd10-6ce0-454f-af78-61530c622b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62ef0d-2fb9-49bc-9233-a6a445d055b3",
   "metadata": {},
   "source": [
    "##### My implementation of vanilla Transformer [пока не надо]\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb0863-d818-4f67-8986-3e26b98a12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sqrt_v = math.sqrt(dim_out) \n",
    "        self.to_qkv = nn.Linear(dim_in, 3*dim_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = einops.rearrange(qkv, \"h (k w) -> k h w\", k=3)\n",
    "\n",
    "        return torch.sigmoid(q @ k.T) @ v / self.sqrt_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48d549-79a5-45e2-a7de-b761a28e81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, heads_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads_num = heads_num\n",
    "        self.heads = [SelfAttention(dim_in, dim_out) for _ in range(heads_num)]\n",
    "        self.epic_w = nn.Linear(heads_num * dim_out, dim_in)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = [head(x) for head in self.heads]\n",
    "        outs = einops.rearrange(outs, \"head h w -> h (head w)\", head=self.heads_num)\n",
    "        x = self.epic_w(outs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb80dfe-4ef5-434b-af96-1b9f721080ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([3, 6])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03db9a7-62e1-4ef7-a4f7-f5184952387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(6, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a581ae-c0cf-40be-b4b9-865137e1b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94d0ba-087a-4689-8b5d-d14e103822ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
