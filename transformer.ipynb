{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d5df28-6f05-4aba-a6f6-8cf703cf9a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11e90b28d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import einops\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import PlaneSet2Neurons, configurate_xy_tensors\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from IPython.display import display\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14178094-e7e3-4058-8a5c-b243505bd691",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2ec10-01d6-48f9-9713-c1a550f90924",
   "metadata": {},
   "source": [
    "Если обучаешь эту модель с нуля, то расскоменть все строчки в ячейке ниже и не выполняй ячейку с _Load model if necessary_. Если хочешь загрузить модель из пикля, то закомменть `model = ViTForImageClass` и не выполняй ячейку _Replace the latest feed-forward layer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dcf0d93-18b0-4cfb-8cae-ae6186c300ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "# model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03068689-76ce-48b8-9ad9-8457f68478a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters amount: 86 567 656\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "print(\"Trainable parameters amount: {:n}\".format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff09e07-d522-48e0-bb0a-f543035072e5",
   "metadata": {},
   "source": [
    "##### Replace the latest feed-forward layer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099655b1-12e1-4f30-98b6-a08f4cace186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=1000, bias=True)\n",
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier)\n",
    "model.classifier = nn.Linear(768, 2)\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68f4a9-5d1b-4c43-a61f-e94c67f11a20",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">Load model if necessary</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd041a7-158f-442f-81c8-9a4fe859c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fine_tuned_transformer_20epo\", \"rb\") as bfile:\n",
    "    model = pickle.load(bfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19523d35-e07e-4f03-8cc5-b1ca5c7269fa",
   "metadata": {},
   "source": [
    "##### Fine tune the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b84cf4-95cc-47dd-b911-fb461b696ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 1\n",
    "batch_size = 8\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device=device)\n",
    "\n",
    "# data\n",
    "# --------------------------\n",
    "csv_path = r\"../train\"\n",
    "images_path = r\"../avia-train/\"\n",
    "with open(csv_path, \"r\") as file:\n",
    "    data = pd.read_csv(file)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train_df = data[msk]\n",
    "test_df = data[~msk]\n",
    "train_dataset = PlaneSet2Neurons(images_path, train_df)\n",
    "test_dataset = PlaneSet2Neurons(images_path, test_df)\n",
    "train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0fa9d4-1c15-4c07-b6b4-142641f1da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch_to_pil(batch):\n",
    "    return [\n",
    "        torchvision.transforms.ToPILImage()(i)\n",
    "        for i in batch\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650781f2-34f2-4735-b127-ccc071b11181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test, batch_size=5):\n",
    "    correct = 0\n",
    "    batch_num = 0\n",
    "\n",
    "    for x, y in iter(test):\n",
    "        x, y = configurate_xy_tensors(x, y)\n",
    "        x = convert_batch_to_pil(x)\n",
    "        x = feature_extractor(images=x, return_tensors=\"pt\")\n",
    "        y = einops.rearrange(y, \"b h w -> b (h w)\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x.pixel_values.cuda())\n",
    "\n",
    "        logits = y_hat.logits\n",
    "\n",
    "        bool_res = np.array(list(map(\n",
    "            lambda x: (x[0] & x[1]).cpu().numpy(), \n",
    "            torch.tensor(y.to(int) == torch.round(logits).to(int))\n",
    "        ))).astype(float)\n",
    "\n",
    "        correct += bool_res.sum()\n",
    "        batch_num += 1\n",
    "\n",
    "    return correct / (batch_num * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecd16a07-0a7f-4737-9f0b-d69f0b12b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958e43aa-de1d-4649-b647-87c5d5f2dfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on 1 epochs with batch_size=8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-611ac04dab80>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y.to(int) == torch.round(logits).to(int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss: 0.10820\n",
      "Wall time: 19min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_dict = {}\n",
    "accuracy_dict = {}\n",
    "\n",
    "print(f\"Run on {num_epochs} epochs with batch_size={batch_size}\\n~time: {round(num_epochs * (19*60+5) / 60, 3)}s\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train_accumulator = []\n",
    "\n",
    "    for ind, (x, y) in enumerate(train):\n",
    "        x, y = configurate_xy_tensors(x, y)\n",
    "        x = convert_batch_to_pil(x)\n",
    "        x = feature_extractor(images=x, return_tensors=\"pt\")\n",
    "        y_hat = model(x.pixel_values.cuda())\n",
    "        logits = y_hat.logits\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y = einops.rearrange(y, \"b h w -> b (h w)\")\n",
    "        loss = distance(logits, y)\n",
    "        diff = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train_accumulator.append(diff)\n",
    "        \n",
    "        del x, y, y_hat, logits, loss\n",
    "        \n",
    "    loss_dict[epoch+1] = np.mean(loss_train_accumulator)\n",
    "    accuracy_dict[epoch+1] = predict(model, test, batch_size)\n",
    "    \n",
    "    # save the model every 5 epoch\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        with open(f\"fine_tuned_transformer_epoch{ind}\", \"wb\") as bfile:\n",
    "            pickle.dump(model, bfile)\n",
    "\n",
    "    print('epoch [{}/{}], loss: {:.5f}'.format(epoch+1, num_epochs, np.mean(loss_train_accumulator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc4cfdd9-ad4e-4f88-ab0f-fc3d4297584b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.8769659239842726}, {1: 0.10819678686916342})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict, loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62ef0d-2fb9-49bc-9233-a6a445d055b3",
   "metadata": {},
   "source": [
    "##### My implementation of vanilla Transformer [пока не надо]\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb0863-d818-4f67-8986-3e26b98a12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sqrt_v = math.sqrt(dim_out) \n",
    "        self.to_qkv = nn.Linear(dim_in, 3*dim_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = einops.rearrange(qkv, \"h (k w) -> k h w\", k=3)\n",
    "\n",
    "        return torch.sigmoid(q @ k.T) @ v / self.sqrt_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48d549-79a5-45e2-a7de-b761a28e81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, heads_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads_num = heads_num\n",
    "        self.heads = [SelfAttention(dim_in, dim_out) for _ in range(heads_num)]\n",
    "        self.epic_w = nn.Linear(heads_num * dim_out, dim_in)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = [head(x) for head in self.heads]\n",
    "        outs = einops.rearrange(outs, \"head h w -> h (head w)\", head=self.heads_num)\n",
    "        x = self.epic_w(outs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb80dfe-4ef5-434b-af96-1b9f721080ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([3, 6])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03db9a7-62e1-4ef7-a4f7-f5184952387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(6, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a581ae-c0cf-40be-b4b9-865137e1b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94d0ba-087a-4689-8b5d-d14e103822ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
